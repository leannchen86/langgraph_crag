# Code adapted from LangChain's Corrective RAG jupyter notebook.

Building a retrieval-augmented generation (RAG) system can be easy, but the harder part often comes from having it work correctly. For example, if wrong information is being selected early on in the retrieval process, it's obvious that the quality of generated answer is going to be bad. To address this issue, Corrective RAG is being explored to more carefully evaluate the quality of information retrieval. However, how effective is it? And is there a one-size-fits-all solution? Carefully watch til the end to find out how you should think more deeply about evaluating your retrieval process.
YouTube Video: https://youtu.be/4RbwR4DRGI8
